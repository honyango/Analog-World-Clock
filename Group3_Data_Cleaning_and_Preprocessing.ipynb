{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/honyango/Analog-World-Clock/blob/master/Group3_Data_Cleaning_and_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Cleaning and Preprocessing**"
      ],
      "metadata": {
        "id": "vB4xstx_j5Pd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Context:**\n",
        "\n",
        "The market for used and refurbished devices has grown significantly over the past decade, as it provides cost-effective alternatives for both consumers and businesses seeking to save money on purchases. By maximizing the longevity of devices through second-hand trade, this market also reduces their environmental impact and aids in recycling and waste reduction."
      ],
      "metadata": {
        "id": "Pth6xr0FkCkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Objective:**\n",
        "To explore the relationships between device specifications and usage patterns, clean and preprocess the data to ensure consistency and completeness, and prepare it for further analysis to inform strategies in the refurbished device market.\n"
      ],
      "metadata": {
        "id": "0-jOL3FXkGuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Description:**\n",
        "\n",
        "The data contains the different data related to a device. The detailed data dictionary is given below.\n",
        "\n",
        "## **Data Dictionary**\n",
        "\n",
        "- **device_brand**: Name of manufacturing brand\n",
        "- **Operating System** (OS): OS on which the device runs\n",
        "- **screen_size**: Size of the screen in cm\n",
        "- **4g**: Whether 4G is available or not\n",
        "- **5g**: Whether 5G is available or not\n",
        "- **rear_camera_mp**: Resolution of the rear camera in megapixels\n",
        "- **front_camera_mp**: Resolution of the front camera in megapixels\n",
        "- **internal_memory**: Amount of internal memory (ROM) in GB\n",
        "- **ram**: Amount of RAM in GB\n",
        "- **battery**: Energy capacity of the device battery in mAh\n",
        "- **weight**: Weight of the device in grams\n",
        "- **release_year**: Year when the device model was released\n",
        "- **days_used**: Number of days the used/refurbished device has been used\n"
      ],
      "metadata": {
        "id": "SmQGU_7AkKMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing the necessary libraries**"
      ],
      "metadata": {
        "id": "Y6zCsIYGkTrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "jutHeNpWOF8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Lt8EalF4MTvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries to help with reading and manipulating data\n",
        "def read_dataset(path):\n",
        "  return pd.read_csv(path)\n",
        "\n",
        "\n",
        "# Libaries to help with data visualization\n",
        "def data_info(df):\n",
        "  return df.info()"
      ],
      "metadata": {
        "id": "o_QGaJNeJEy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Brief explaination of the use of each library**\n",
        "\n",
        "**Numpy:**\n",
        "\n",
        "Numpy is used for handling Numbers, Numerical analysis. It is the fundamental package for array computing with Python.\n",
        "\n",
        "**Pandas:**\n",
        "\n",
        "Pandas are used to process the data. Pandas contain data structures and data manipulation tools designed for data cleaning and analysis.\n",
        "\n",
        "\n",
        "**matplotlib.pyplot:**\n",
        "\n",
        "Matplotlib is a visualization library & has been taken from the software `Matlab`. We are only considering one part of this library to show plotting, hence used .pyplot which means python plot.\n",
        "\n",
        "**Seaborn:**\n",
        "\n",
        "Seaborn is another visualization library. When it comes to the visualization of statistical models like heat maps, Seaborn is among the reliable sources. This Python library is derived from matplotlib and closely integrated with Pandas data structures.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "unXVJ-thkzgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Read the dataset**\n",
        "\n",
        "We use the Pandas library to load the dataset from a CSV file. Pandas provides efficient tools for handling structured data, making it easy to analyze and manipulate."
      ],
      "metadata": {
        "id": "WIVQZYpNlSJM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wj4Fyp_UNR1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIaoQVmiju2s"
      },
      "outputs": [],
      "source": [
        "# Read dataset\n",
        "def read_dataset(path):\n",
        "  return pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Overview of the dataset**\n",
        "\n",
        "Show the first 10 records of the dataset. How many columns are there?"
      ],
      "metadata": {
        "id": "z2vbIMH6k5Wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns or attributes\n",
        "def columns(df):\n",
        "  return df.columns"
      ],
      "metadata": {
        "id": "O5XG7ItZKzYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overview %% Columns or attributes %% [markdown] The dimension of the dataset?\n",
        "# Find the dimension of the dataframe.\n",
        "#\n",
        "# - The **shape** of the dataset is a **tuple of 2 elements**. %% Shape of the\n",
        "# dataset %% [markdown] The size of the dataset? Find the size of the dataframe.\n",
        "#\n",
        "# - The **size** of the dataset is the **total number of elements** in the data\n",
        "# i.e. product of the number of rows and number of columns. %% The size of the\n",
        "# dataset %% [markdown] ## **Exploratory data analysis**\n",
        "#\n",
        "# **Exploratory data analysis** (EDA) is used for analyzing and investigating\n",
        "# datasets and summarizing their main characteristics, often employing data\n",
        "# visualization methods.\n",
        "#\n",
        "# EDA is an important first step in any data analysis. %% [markdown] What are\n",
        "# the data types of all the variables in the data set?"
      ],
      "metadata": {
        "id": "iNBoA8L9k7GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dimension of the dataset? Find the dimension of the dataframe.\n",
        "\n",
        "- The **shape** of the dataset is a **tuple of 2 elements**."
      ],
      "metadata": {
        "id": "pnGCk5BbljyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of the dataset\n",
        "def shape(df):\n",
        "  return df.shape"
      ],
      "metadata": {
        "id": "kdQrHyCYlli1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The size of the dataset? Find the size of the dataframe.\n",
        "\n",
        "- The **size** of the dataset is the **total number of elements** in the data i.e. product of the number of rows and number of columns."
      ],
      "metadata": {
        "id": "em6FXvddl3Tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The size of the dataset\n",
        "def size(df):\n",
        "  return df.size"
      ],
      "metadata": {
        "id": "YPgw08nNl50o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exploratory data analysis**\n",
        "\n",
        "**Exploratory data analysis** (EDA) is used for analyzing and investigating datasets and summarizing their main characteristics, often employing data visualization methods.\n",
        "\n",
        "EDA is an important first step in any data analysis."
      ],
      "metadata": {
        "id": "fnWlHn4JNCw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the data types of all the variables in the data set?\n",
        "\n",
        "**Hint: Use the `info()` function to get all the information about the dataset.**"
      ],
      "metadata": {
        "id": "5pVTNIHFl9in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Data info/data types\n",
        "def data_info(df):\n",
        "  return df.info()"
      ],
      "metadata": {
        "id": "sLNAMOxel-Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write your observations.\n",
        "\n",
        "**Observations**\n",
        "\n",
        "-\n",
        "it shows all the information in the data set\n",
        "-\n",
        "\n",
        "What are the qualitative and quatitative variables?\n",
        "\n",
        "Quantitative Variables → Numerical can be measured or counted\n",
        "\n",
        "internal_memory, battery\n",
        "\n",
        "Qualitative Variables → Categorical labels, categories, non-numeric types\n",
        "\n",
        "brand, model, colour"
      ],
      "metadata": {
        "id": "bQaimwGmMF9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do we mean by missing values? Are there any missing values in the dataframe?"
      ],
      "metadata": {
        "id": "uuXe1PvUmJ1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Any missing values?\n",
        "def missing_values(df):\n",
        "  return df.isnull().sum()"
      ],
      "metadata": {
        "id": "74keNckKmNNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "-It shows all the information in the data set -\n",
        "\n",
        "What are the qualitative and quatitative variables?\n",
        "\n",
        "Quantitative Variables → Numerical can be measured or counted\n",
        "\n",
        "internal_memory, battery\n",
        "\n",
        "Qualitative Variables → Categorical labels, categories, non-numeric types\n",
        "\n",
        "brand, model, colour\n",
        "\n",
        "-\n"
      ],
      "metadata": {
        "id": "Aq_GFf2_tQhb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do summary statistics of data represent? Find the summary statistics for the numerical variables (Dtype is int64) in the data?\n",
        "\n",
        "\n",
        "Prediction Models: Regression, machine learning, or deep learning models can be used to predict and replace missing values based on other data.\n",
        "Handling missing values is an essential part of the data cleaning process in data science. Understanding why data is missing and identifying the mechanism by which it is missing helps in deciding the most appropriate method to handle such missing values."
      ],
      "metadata": {
        "id": "YY5d60xQms7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistics summary"
      ],
      "metadata": {
        "id": "rT98uA6omufF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- What is the central tendency of the data?\n",
        "    - What is the average screen size of the refurbished devices?\n",
        "    - What is the median battery capacity (in mAh) of the devices?\n",
        "\n",
        "- How spread out is the data?\n",
        "    - What's the range (minimum to maximum) of internal memory (ROM) sizes available?\n",
        "    - What's the standard deviation of device weights?\n",
        "\n",
        "- Are there any outliers or extreme values?\n",
        "    - What are the minimum and maximum values for each variable?\n",
        "\n",
        "- What are the average resolutions for rear and front cameras?\n",
        "\n",
        "- What's the average number of days these refurbished devices have been used?"
      ],
      "metadata": {
        "id": "uJ26Y9YlN-Dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EPyr5vUuQGSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answers:**\n",
        "13.713115 Average Mean\n",
        "-\n",
        "3000 mah\n",
        "-Standard deviation weight\n",
        "88.413228"
      ],
      "metadata": {
        "id": "ujYnyFq1tbAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summary_statistics(df):\n",
        "  return df.describe()"
      ],
      "metadata": {
        "id": "9fLlviE9UoAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unique_values(df):\n",
        "  return df.nunique()"
      ],
      "metadata": {
        "id": "Xxk9XpYhVUNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify any unique values for categorical attributes\n",
        "def unique_values(df):\n",
        "  return df.nunique()"
      ],
      "metadata": {
        "id": "zuy7gzXE9RrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def value_counts(df):\n",
        "  return df.value_counts()"
      ],
      "metadata": {
        "id": "EXM6rahNfgq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def value_counts(df):\n",
        "  return df.value_counts()"
      ],
      "metadata": {
        "id": "LPkB8YgEe_-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "date_range = pd.date_range(start='2023-01-01', end='2023-12-31')"
      ],
      "metadata": {
        "id": "hQCMfa5HXC4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_date_range(start_date, end_date):\n",
        "    date_range = pd.date_range(start=start_date, end=end_date)\n",
        "    return date_range"
      ],
      "metadata": {
        "id": "rLV1INvHVHWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Cleaning**\n",
        "\n",
        "Data cleaning is a crucial step in the data preprocessing pipeline, with handling missing values being a key component. Here are several techniques to address missing data:\n",
        "\n",
        "### **1. Handle missing values**\n",
        "\n",
        "- Deletion: Remove rows with missing values if few.\n",
        "    - Listwise deletion: Remove entire records (rows) containing any missing values\n",
        "    - Pairwise deletion: Use available data while ignoring missing data during analysis, particularly useful in statistical calculations like correlation\n",
        "\n",
        "- Mean/Median/Mode Imputation: Replace with average or middle value or most frequent (for categorical variables).\n",
        "- Forward fill (ffill) and backward fill (bfill) Imputations: They are imputation techniques that use the values from previous or next observations to fill in the missing values. This can be applied with **time-series** dataset.\n",
        "- Using Domain Knowledge: In some cases, subject-matter expertise can be used to make educated guesses about missing values.\n",
        "- Machine Learning Approaches\n",
        "\n",
        "### When choosing a method to handle missing values, consider:\n",
        "\n",
        "- The proportion of missing data (e.g., if >30%, consider removing rows).\n",
        "\n",
        "- The nature of the missing data (completely at random, at random, or not at random).\n",
        "\n",
        "-  The potential impact on your analysis or model.\n",
        "\n",
        "- The distribution of the data (e.g., use median for skewed distributions, mean for normal distributions)\n",
        "\n",
        "\n",
        "In datasets, missing entries might appear as the letter \"0\", \"NA\", \"NaN\", \"NULL\", \"Not Applicable\", or \"None”."
      ],
      "metadata": {
        "id": "FhYKiefaP1tI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlAG0Cs5alCk",
        "outputId": "53d2c15c-dc7e-4db2-9cc6-b29e16d56928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TLZPwKYfamlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Deletion\n",
        "# Listwise deletion: Remove rows with any missing values\n",
        "def listwise_deletion(df):\n",
        "  return df.dropna()\n"
      ],
      "metadata": {
        "id": "sT580NwXMoFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def listwise_deletion(df):\n",
        "  return df.dropna()"
      ],
      "metadata": {
        "id": "6bpLc1mdidvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Mean/Median/Mode Imputation\n",
        "# Replace numerical columns with mean/median\n",
        "def mean_imputation(df):\n",
        "  return df.fillna(df.mean(), inplace=True)"
      ],
      "metadata": {
        "id": "EfYUcwJxufeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do same for the columns `internal_memory` and `battery`."
      ],
      "metadata": {
        "id": "CnWm60TJvkl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def median_imputation(df):\n",
        "  return df.fillna(df.median(), inplace=True)"
      ],
      "metadata": {
        "id": "MpLx0qE_jR4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def mode_imputation(df):\n",
        "  return df.fillna(df.mode(), inplace=True)"
      ],
      "metadata": {
        "id": "NBy7ljxKjRhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_fill(df):\n",
        "  return df.fillna(method='ffill', inplace=True)"
      ],
      "metadata": {
        "id": "EtLt45GToBvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_fill(df):\n",
        "  return df.fillna(method='bfill', inplace=True)"
      ],
      "metadata": {
        "id": "KcZ3F_3VwVkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def domain_knowledge_imputation(df):\n",
        "  return df.fillna(value, inplace=True)"
      ],
      "metadata": {
        "id": "1jUIuyX_mopK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Detect and handle duplicates**\n",
        "\n",
        "​Detecting and handling duplicate data is essential for maintaining the integrity of your dataset."
      ],
      "metadata": {
        "id": "_UKPpdF9wZNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect duplicate rows\n",
        "def detect_duplicates(df):\n",
        "  return df[df.duplicated()]"
      ],
      "metadata": {
        "id": "o4QVuLZewYdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View duplicates\n",
        "def view_duplicates(df):\n",
        "  return df[df.duplicated(keep=False)]"
      ],
      "metadata": {
        "id": "sN5AJnMCxjHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates\n",
        "def remove_duplicates(df):\n",
        "  df.drop_duplicates(inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "THa_Vo5Mxpcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Detect and handle outliers**\n",
        "\n",
        "**Outliers** are data points that deviate significantly from the normal distribution or expected trends within a dataset in the context of data analysis. These anomalous points can introduce noise, skew statistical measurements, and reduce the accuracy of analytical models.\n",
        "\n",
        "As a result, identifying and dealing with outliers is crucial for generating trustworthy insights and making data-driven decisions. Outliers can take numerous forms, including extreme values, anomalies, and data-gathering errors.\n",
        "\n",
        "\n",
        "### **Outlier detection techniques**\n",
        "Outlier detection techniques vary in their advantages, limitations, and underlying assumptions. Therefore, it is crucial to select the appropriate method based on the specific characteristics of your data, the objectives of your analysis, and the requirements of your project.\n",
        "\n",
        "- Statistical Method: Use IQR to identify outliers\n",
        "- Z-Score: Flag values beyond ±3 standard deviations from the mean (assumes normal distribution)\n",
        "- Visual Method: Use boxplots to visualize outliers."
      ],
      "metadata": {
        "id": "AifqauTebqGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Z_uMqP4_rw42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m_03pAcVrwsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wJzATcXIrxxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_outliers(df, column):\n",
        "  Q1 = df[column].quantile(0.25)\n",
        "  Q3 = df[column].quantile(0.75)\n",
        "  IQR = Q3 - Q1"
      ],
      "metadata": {
        "id": "Mtiiqe9urbk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to detect outliers using IQR\n",
        "def iqr (data, column):\n",
        "  q1 = data[column].quantile(0.25)\n",
        "  q3 = data[column].quantile(0.75)\n",
        "  iqr = q3 - q1"
      ],
      "metadata": {
        "id": "aW6DW3DCcEOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_outliers(df, column):\n",
        "  lower_bound = Q1 - 1.5 * IQR\n",
        "  upper_bound = Q3 + 1.5 * IQR"
      ],
      "metadata": {
        "id": "Ps7ngMhiyfnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to detect outliers using zscore\n",
        "def zscore(data, column):\n",
        "  mean = data[column].mean()\n",
        "  std = data[column].std()"
      ],
      "metadata": {
        "id": "HsQMSYurzsbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocessing**\n",
        "**Data preprocessing** is a critical step in data analytics and machine learning, involving the transformation of raw, unstructured, or incomplete data into a clean, usable format. This ensures that data is accurate, consistent, and ready for analysis or model training.\n",
        "\n",
        "### **1. Data Normalization**\n",
        "This is a fundamental preprocessing technique in data analytics and machine learning. It involves adjusting the scales of numerical features to ensure that each contributes equally to the analysis, preventing features with larger ranges from dominating the mode.\n",
        "\n",
        "1.\t**Min-Max Scaling (Rescaling)**: This method transforms features to fit within a specified range, typically [0, 1]. It's particularly useful when the data distribution is uniform and does not contain outliers.\n",
        "\n",
        "$$x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}$$\n",
        "\n",
        "2.\t**Z-Score Normalization (Standardization)**: This technique centers the data around zero with a standard deviation of one, effectively converting the data into a standard normal distribution. It's suitable when the data follows a normal distribution and is less affected by outliers.\n",
        "\n",
        "$$x' = \\frac{x - \\mu}{\\sigma}$$\n",
        "\n",
        "3.\t**Log Scaling**: This approach applies the natural logarithm to the data, which can be beneficial when dealing with data that spans several orders of magnitude or exhibits exponential growth. It's particularly effective for data that follows a power law distribution.\n",
        "\n",
        "$$x' = \\log(x)$$"
      ],
      "metadata": {
        "id": "cmZgS6AQg3En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n"
      ],
      "metadata": {
        "id": "A4OFLkWJJfKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def min_max_scaling(data, column):\n",
        "  scaler = MinMaxScaler()\n",
        "  data[column] = scaler.fit_transform(data[[column]])"
      ],
      "metadata": {
        "id": "WZSlJci6KTbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Min-Max normalization\n",
        "def min_max_normalization(data, column):\n",
        "  min_value = data[column].min()\n",
        "  max_value = data[column].max()"
      ],
      "metadata": {
        "id": "dI_3EwLxhPHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Encode categorical variables**\n",
        "\n",
        "​Encoding categorical variables is a crucial preprocessing step in data analysis and machine learning. It involves transforming non-numeric data into a numerical format that algorithms can interpret effectively.\n",
        "\n",
        "1. **Label Encoding**: This is suitable for **ordinal data** where the categories have a meaningful order. However, use caution with nominal data, as the numerical codes may imply an unintended ordinal relationship.\n",
        "\n",
        "2. **One-Hot Encoding**: This is preferred for **nominal data** without an inherent order, as it prevents the introduction of ordinal relationships. Be mindful that it can increase the dimensionality of the dataset, especially with features containing many unique categories."
      ],
      "metadata": {
        "id": "7t54lNlshVz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot encoding\n",
        "def one_hot_encoding(data, column):\n",
        "  encoded_data = pd.get_dummies(data[column], prefix=column)\n",
        "  data = pd.concat([data, encoded_data], axis=1)\n"
      ],
      "metadata": {
        "id": "cHAJSaGBhZrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_encoding(data, column):\n",
        "  label_encoder = LabelEncoder()\n",
        "  data[column] = label_encoder.fit_transform(data[column])\n",
        "#"
      ],
      "metadata": {
        "id": "YzRpcotchjVd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}